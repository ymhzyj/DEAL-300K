## **DEAL-300K**: Diffusion-based Editing Area Localization with a 300K-Scale Dataset and Frequency-Prompted Baseline



## Description


This repository serves as the official repository for DEAL-300K. It contains the DEAL-300K dataset along with other tools mentioned in the paper. The code and related pre-trained models will be released after the paper is accepted.





## TODO List
- [x] Release DEAL-300K dataset.
- [ ] Release SAM-CD pre-trained weights.
- [ ] Release Qwen-VL pre-trained weights.
- [ ] Release MFPT pre-trained weights.
- [ ] Release Full Code.

## Overview
The advent of diffusion-based image editing techniques has revolutionized image manipulation, providing intuitive, semantic-level editing capabilities. These advancements significantly lower the barrier for non-experts to produce high-quality edits but also raise concerns regarding potential misuse. Traditional datasets, primarily focused on binary classification of diffusion-generated images or localization of manual manipulations, do not address the challenges posed by diffusion-based edits, which blend seamlessly with the original content. In response, we introduce the Diffusion-Based Image Editing Area Localization Dataset (DEAL-300K), a novel dataset comprising over 300,000 annotated images specifically designed for diffusion-based image manipulation localization (DIML). Our dataset generation leverages multimodal large language models (MLLMs) for instruction-driven editing, combined with an active learning annotation process, ensuring both diversity and quality at an unprecedented scale. Additionally, we present a novel benchmarking approach that combines Visual Foundation Models (VFMs) with Multi-Frequency Prompt Tuning (MFPT), capturing the intricate details of diffusion-edited regions. Our thorough evaluation highlights the effectiveness of our method, achieving an impressive pixel-level F1 score of $82.56\%$ on our specialized test set and $80.97\%$ on the external CoCoGlide set, demonstrating its strong performance across different datasets. 

## DEAL-300K dataset

### Download
The dataset has been uploaded to OneDrive. 

Training Set Images can be downloaded from [train.zip](https://1drv.ms/u/s!AgVmq0AY0Su8gr1DmG-HWFsws_Fkww?e=MOPn47)

Val Set Images can be downloaded from [val.zip](https://1drv.ms/u/s!AgVmq0AY0Su8grxy38macOANthNBfA?e=0jgmly)

Testing Set Images can be downloaded from [test.zip](https://1drv.ms/u/s!AgVmq0AY0Su8grxwCKrru0NLUkf39g?e=DNfNRK)

Labels can be downloaded from [label.zip](https://1drv.ms/u/s!AgVmq0AY0Su8grxx2lnMXYsGYhv79w?e=nGRYJt)

### Instructions

Our dataset is based on InstructionPix2Pix, with all instructions generated by the fine-tuned Qwen-VL. You can view all the instructions in [instructions](./instructions). The original images come from the MS COCO dataset. 
The word cloud of the editing instructions is shown in the image below.

<!-- ![Word Cloud](assets/wordcloud-page-001.jpg) -->
<div style="text-align: center;">
<img src="assets/wordcloud-page-001.jpg" alt="Word Cloud" width="500">
</div>


### Quantitative comparison of DEAL-300K to existing publicly available DIML dataset.

| Dataset | Year | Source Images  | Edited Images | Image Size | Scenario | Generative Model | 
|------------------|-------------------------------------------------------------|---------------------|---------------------|-----|-----|------|
| [CoCoGlide](https://github.com/grip-unina/TruFor) | 2023 | 512                  | 512    | $256 \times 256$  | General   | Glide   | 
| [AutoSplice](https://github.com/shanface33/AutoSplice_Dataset)| 2023 | 2,273                 | 3,621 | $256 \times 256 - 4232 \times 4232$  | General   | DALL-E2   |
| [MagicBrush](https://github.com/OSU-NLP-Group/MagicBrush)| 2023 | 5,313               | 10,388  | $1024 \times 1024$  | General   | DALL-E2   |
| [Repaint-P2/CelebA-HQ](https://github.com/bit-ml/dolos) | 2024 | 10,800  | 41,472   | $256 \times 256$   | Face   | Repaint   |
| DEAL-300K | 2024 Apr  | 119,371  | 221,097       | $128 \times 512 - 512 \times 576$  | General  | InstructPix2Pix   |

### Visualization

Some random examples from the training set
<div style="display: flex; justify-content: center;">
    <img src="assets/examples/000000000009_qwen120.jpg" alt="Image ori 1" width="300">
    <img src="assets/examples/000000000009.jpg" alt="Image ori 2" width="300">
    <img src="assets/examples/000000000009_qwen120.png" alt="Image ori 2" width="300">
</div>

<div style="display: flex; justify-content: center;">
    <img src="assets/examples/000000000459_qwen120.jpg" alt="Image ori 1" width="300">
    <img src="assets/examples/000000000459.jpg" alt="Image ori 2" width="300">
    <img src="assets/examples/000000000459_qwen120.png" alt="Image ori 2" width="300">
</div>


<div style="display: flex; justify-content: center;">
    <img src="assets/examples/000000436603_qwen120.jpg" alt="Image ori 1" width="300">
    <img src="assets/examples/000000436603.jpg" alt="Image ori 2" width="300">
    <img src="assets/examples/000000436603_qwen120.png" alt="Image ori 2" width="300">
</div>

<div style="display: flex; justify-content: center;">
    <img src="assets/examples/000000436647_qwen120.jpg" alt="Image ori 1" width="300">
    <img src="assets/examples/000000436647.jpg" alt="Image ori 2" width="300">
    <img src="assets/examples/000000436647_qwen120.png" alt="Image ori 2" width="300">
</div>

<div style="display: flex; justify-content: center;">
    <img src="assets/examples/000000436795_qwen180.jpg" alt="Image ori 1" width="300">
    <img src="assets/examples/000000436795.jpg" alt="Image ori 2" width="300">
    <img src="assets/examples/000000436795_qwen180.png" alt="Image ori 2" width="300">
</div>

<div style="display: flex; justify-content: center;">
    <img src="assets/examples/000000436797_qwen180.jpg" alt="Image ori 1" width="300">
    <img src="assets/examples/000000436797.jpg" alt="Image ori 2" width="300">
    <img src="assets/examples/000000436797_qwen180.png" alt="Image ori 2" width="300">
</div>

## Acknowledgments
Our work is built upon the foundational work of [MS COCO](https://cocodataset.org/), [InstructPix2Pix](https://github.com/timothybrooks/instruct-pix2pix), [Qwen-VL](https://github.com/QwenLM/Qwen-VL), [ISAT](https://github.com/yatengLG/ISAT) and [SAM-CD](https://github.com/ggsDing/SAM-CD/tree/main).

